(window.webpackJsonp=window.webpackJsonp||[]).push([[22],{514:function(t,a,s){"use strict";s.r(a);var _=s(55),n=Object(_.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("p",[t._v("校内选修课《深度学习实战》的作业之一。基于CNN的模型进行猫狗图像分类，训练集为8000张猫狗图片，最终准确率达到93.6%")]),t._v(" "),s("h1",{attrs:{id:"基于cnn进行猫狗图像分类"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#基于cnn进行猫狗图像分类"}},[t._v("#")]),t._v(" 基于CNN进行猫狗图像分类")]),t._v(" "),s("p",[s("img",{attrs:{src:"http://test-123-imagebed.oss-cn-beijing.aliyuncs.com/img/cats&dogs.jpg",alt:"cats&dogs"}})]),t._v(" "),s("blockquote",[s("p",[t._v("图源：https://www.skyfilabs.com/project-ideas/image-classifier-for-identifying-cats-dogs")])]),t._v(" "),s("h2",{attrs:{id:"前置知识"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#前置知识"}},[t._v("#")]),t._v(" 前置知识")]),t._v(" "),s("h3",{attrs:{id:"猫狗数据集"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#猫狗数据集"}},[t._v("#")]),t._v(" 猫狗数据集")]),t._v(" "),s("p",[t._v("狗 vs 猫数据集是指用于 2013 年举行的 Kaggle 机器学习竞赛的数据集。")]),t._v(" "),s("p",[t._v("该数据集由狗和猫的照片组成，作为来自 300 万张手动注释照片的更大数据集的照片子集提供。该数据集是作为 Petfinder.com 和 Microsoft 之间的合作伙伴关系开发的。")]),t._v(" "),s("p",[t._v("该数据集最初用作 CAPTCHA（或完全自动化的公共图灵测试，以区分计算机和人类），即认为人类认为微不足道但机器无法解决的任务，用于网站上区分在人类用户和机器人之间。")]),t._v(" "),s("p",[t._v("具体来说，该任务被称为“ "),s("em",[t._v("Asirra")]),t._v(" ”或用于限制访问的动物物种图像识别，一种 CAPTCHA。该任务在 2007 年题为“ "),s("a",{attrs:{href:"https://www.microsoft.com/en-us/research/publication/asirra-a-captcha-that-exploits-interest-aligned-manual-image-categorization/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Asirra: A CAPTCHA that Exploits Interest-Aligned Manual Image Categorization")]),t._v(" ”中有所描述。")]),t._v(" "),s("blockquote",[s("p",[t._v("我们展示了 Asirra，这是一个 CAPTCHA，它要求用户从一组 12 张猫和狗的照片中识别出猫。Asirra 对用户来说很容易；用户研究表明，人类可以在 30 秒内解决 99.6% 的问题。除非机器视觉取得重大进展，否则我们预计计算机解决它的机会不会超过 1/54,000。")]),t._v(" "),s("p",[t._v("— "),s("a",{attrs:{href:"https://www.microsoft.com/en-us/research/publication/asirra-a-captcha-that-exploits-interest-aligned-manual-image-categorization/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Asirra：利用兴趣对齐的手动图像分类的验证码")]),t._v("，2007 年。")])]),t._v(" "),s("h3",{attrs:{id:"cnn的相关概念"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#cnn的相关概念"}},[t._v("#")]),t._v(" CNN的相关概念")]),t._v(" "),s("h4",{attrs:{id:"什么是cnn"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#什么是cnn"}},[t._v("#")]),t._v(" 什么是CNN")]),t._v(" "),s("p",[t._v("卷积神经网络（convolutional neural network，CNN）是一类强大的神经网络，正是为处理图像数据而设计的。")]),t._v(" "),s("blockquote",[s("p",[t._v("基于卷积神经网络结构的模型在计算机视觉领域中已经占主导地位，当今几乎所有的图像识别、对象检测或语义分割相关的学术竞赛、商业应用都以这种方法为基础。")])]),t._v(" "),s("p",[t._v("现代卷积神经网络的设计得益于生物学、群论和大量的实验研究。除了在获得精确模型的采样效率外，卷积神经网络在计算上也是极其高效的。这是因为卷积神经网络需要的参数比多层感知机少，而且卷积神经网络很容易用GPU并行计算。")]),t._v(" "),s("h4",{attrs:{id:"cnn的核心层"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#cnn的核心层"}},[t._v("#")]),t._v(" CNN的核心层")]),t._v(" "),s("p",[t._v("在卷积神经网络（Convolutional Neural Network，CNN）中，往往包含许多种不同的网络层交替组成，主要有卷积层（Convolutional Layer）、池化层（Pooling Layer）、非线性层（ReLU Layer）、全连接层（Fully Connected Layer）等等")]),t._v(" "),s("p",[s("img",{attrs:{src:"http://test-123-imagebed.oss-cn-beijing.aliyuncs.com/img/image-20210625094614725.png",alt:"image-20210625094614725"}})]),t._v(" "),s("p",[t._v("Keras中有介绍，"),s("a",{attrs:{href:"https://keras.io/zh/layers/core/",target:"_blank",rel:"noopener noreferrer"}},[t._v("中文文档")]),t._v("。")]),t._v(" "),s("h5",{attrs:{id:"卷积层-convolutional-layer"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#卷积层-convolutional-layer"}},[t._v("#")]),t._v(" 卷积层（Convolutional Layer）")]),t._v(" "),s("p",[t._v("卷积层有时也被称为特征提取器层，因为图像的特征是在该层中提取的。主要对输入"),s("code",[t._v("input volume")]),t._v("进行卷积操作，同时根据参数"),s("code",[t._v("stride")]),t._v("的设置，确定卷积核每两次卷积操作直接滑动的距离。卷积核（kernel）类似一个图像滤波器（filter），每一个卷积核用一个矩阵表示，将其在图片上按照间隔大小滑动处理一遍得到的结果，即为卷积层的activation map（或feature map）。具体卷积操作类似下图。")]),t._v(" "),s("p",[s("img",{attrs:{src:"http://test-123-imagebed.oss-cn-beijing.aliyuncs.com/img/20160907213341798",alt:"img"}})]),t._v(" "),s("p",[t._v("可以用keras实现，卷积操作分为一维、二维、三维，分别为Conv1D，Conv2D，Conv3D。一维卷积主要应用于时间序列数据或者是文本数据，二维卷积通常应用于图像数据。由于这三种的使用和参数基本相同，所以主要以处理图像数据的Conv2D进行说明。")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://keras.io/api/layers/convolution_layers/convolution2d/",target:"_blank",rel:"noopener noreferrer"}},[t._v("keras参数")])]),t._v(" "),s("div",{staticClass:"language-PYTHON extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("tf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Conv2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("filters"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("kernel_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("strides"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("padding"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'valid'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("ul",[s("li",[s("code",[t._v("filers")]),t._v("：卷积核个数")]),t._v(" "),s("li",[s("code",[t._v("kernel_size")]),t._v("：卷积核大小")]),t._v(" "),s("li",[s("code",[t._v("strides")]),t._v("：步长值，二维中默认为(1,1)，一维默认1")]),t._v(" "),s("li",[s("code",[t._v("padding")]),t._v("：补‘0’策略，"),s("code",[t._v("'valid'")]),t._v("为no padding，指卷积大小和原来的大小可以不同，"),s("code",[t._v("'same'")]),t._v("指输出和输出大小相同。")])]),t._v(" "),s("h5",{attrs:{id:"relu-rectified-linear-units-layers"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#relu-rectified-linear-units-layers"}},[t._v("#")]),t._v(" ReLU(Rectified Linear Units) Layers")]),t._v(" "),s("p",[t._v("在每一个卷积层之后，一般会紧接着使用一个非线性层。该非线性层的主要目的是在系统中引入非线性特征。最常见的CNN网络中使用ReLU层，其具有比"),s("code",[t._v("tanh")]),t._v("和"),s("code",[t._v("sigmoid")]),t._v("函数更好的速度与效率，ReLU层主要是对于input的所有值应用函数"),s("span",{staticClass:"katex"},[s("span",{staticClass:"katex-mathml"},[s("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[s("semantics",[s("mrow",[s("mi",[t._v("f")]),s("mo",{attrs:{stretchy:"false"}},[t._v("(")]),s("mi",[t._v("x")]),s("mo",{attrs:{stretchy:"false"}},[t._v(")")]),s("mo",[t._v("=")]),s("mi",[t._v("m")]),s("mi",[t._v("a")]),s("mi",[t._v("x")]),s("mo",{attrs:{stretchy:"false"}},[t._v("(")]),s("mn",[t._v("0")]),s("mo",{attrs:{separator:"true"}},[t._v(",")]),s("mi",[t._v("x")]),s("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),s("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("f(x) = max(0, x)")])],1)],1)],1),s("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[s("span",{staticClass:"base"},[s("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),s("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.10764em"}},[t._v("f")]),s("span",{staticClass:"mopen"},[t._v("(")]),s("span",{staticClass:"mord mathnormal"},[t._v("x")]),s("span",{staticClass:"mclose"},[t._v(")")]),s("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}}),s("span",{staticClass:"mrel"},[t._v("=")]),s("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2777777777777778em"}})]),s("span",{staticClass:"base"},[s("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),s("span",{staticClass:"mord mathnormal"},[t._v("ma")]),s("span",{staticClass:"mord mathnormal"},[t._v("x")]),s("span",{staticClass:"mopen"},[t._v("(")]),s("span",{staticClass:"mord"},[t._v("0")]),s("span",{staticClass:"mpunct"},[t._v(",")]),s("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.16666666666666666em"}}),s("span",{staticClass:"mord mathnormal"},[t._v("x")]),s("span",{staticClass:"mclose"},[t._v(")")])])])]),t._v(" ，也就是说该层可以使所有的negative activations为0，大大地减少储存空间。")]),t._v(" "),s("h5",{attrs:{id:"池化层pooling-layers"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#池化层pooling-layers"}},[t._v("#")]),t._v(" 池化层Pooling Layers")]),t._v(" "),s("p",[t._v("在ReLU层后一般为pooling层，即降采样层，最常见的是maxpooling方法，例如选择滤波器大小为2*2，stride（滑动间隔）与滤波器长度一致，是比较常见的结果。Pooling层对输入矩阵的一块区域整体进行操作，只在该区域保留一个数值，而不同的pooling方法决定该数值的计算方法，maxpooling层对于一个区域输出最大值，同时还有average pooling和L2-norm pooling等方式。pooling操作的示意图如下：")]),t._v(" "),s("p",[s("img",{attrs:{src:"http://test-123-imagebed.oss-cn-beijing.aliyuncs.com/img/image-20210625093349390.png",alt:"image-20210625093349390"}})]),t._v(" "),s("p",[t._v("池化层用于减少卷积后输入图像的空间体积。它用于两个卷积层之间。如果我们在 Convo 层之后直接应用 FC 而不应用池化或最大池化，那么它的计算成本将会很高。")]),t._v(" "),s("p",[t._v("因此，最大池化是减少输入图像空间体积的唯一方法。在上面的例子中，我们在 Stride 为 2 的单深度切片中应用了最大池化。您可以观察到 4 x 4 维度的输入被减少到 2 x 2 维度。")]),t._v(" "),s("h5",{attrs:{id:"全连接层fully-connected-layer"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#全连接层fully-connected-layer"}},[t._v("#")]),t._v(" 全连接层Fully Connected Layer")]),t._v(" "),s("p",[t._v("全连接层一般位于CNN网络的最后部分，该层的输入可以是卷积层，ReLU层，Pooling层的结果，输出一个N维向量，N即为要识别的类别数。例如在在softmax方法下，结果向量是[0  0.1  0.1  0.75  0  0  0  0  0  0.05]，则说明他有10%的概率是第一类，10%的概率是第二类，75%的概率是第三类，5%的概率是第九类。")]),t._v(" "),s("p",[s("img",{attrs:{src:"http://test-123-imagebed.oss-cn-beijing.aliyuncs.com/img/image-20210625093944430.png",alt:"image-20210625093944430"}})]),t._v(" "),s("h5",{attrs:{id:"flatten-layers"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#flatten-layers"}},[t._v("#")]),t._v(" Flatten layers")]),t._v(" "),s("p",[t._v("Flatten层用来将输入“压平”，即把多维的输入一维化，常用在从卷积层到全连接层的过渡。")]),t._v(" "),s("p",[t._v("Flatten不影响batch的大小。将输入展平。即把多维的输入一维化，常用在从卷积层到全连接层的过渡，不影响批量大小。展平后神经元的数目为 (w"),s("em",[t._v("h")]),t._v("chanels,分别前一个为卷积核的宽、高、channel 数)")]),t._v(" "),s("h5",{attrs:{id:"dropout-layers"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#dropout-layers"}},[t._v("#")]),t._v(" Dropout Layers")]),t._v(" "),s("p",[t._v("在全连接层中，特征向量的长度往往都是非常大的。这样在进行计算时就会有非常大的计算量，与此同时，在进行batch training时计算量就会很容易达到一个不能接受的程度。")]),t._v(" "),s("p",[t._v("为了降低训练时的计算负担和防止过度拟合问题出现，Hinton引入了dropout的方法，即在训练时随机的选取一部分参数（可采用将一部分参数置零的方式）进行训练。但是在进行预测时需要将所有参数计入计算范围。该层使用下，使得网络在随机去掉一些激活值后依然能够有正确的分类等结果，即对不同有更好的适应性。具体原理等可以看其文章："),s("a",{attrs:{href:"https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Dropout Paper")]),t._v("。")]),t._v(" "),s("p",[t._v("Keras文档：https://keras.io/api/layers/regularization_layers/dropout/")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/",target:"_blank",rel:"noopener noreferrer"}},[t._v("如何在 Keras 中使用 Dropout 正则化减少过拟合")])]),t._v(" "),s("h3",{attrs:{id:"cnn的训练步骤"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#cnn的训练步骤"}},[t._v("#")]),t._v(" CNN的训练步骤")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("准备训练数据")])]),t._v(" "),s("li",[s("p",[t._v("创建模型")])]),t._v(" "),s("li",[s("p",[t._v("设置模型参数")])]),t._v(" "),s("li",[s("p",[t._v("训练")])]),t._v(" "),s("li",[s("p",[t._v("评估模型")])]),t._v(" "),s("li",[s("p",[t._v("根据评估结果对模型进行优化(fine tune)")])])]),t._v(" "),s("h2",{attrs:{id:"搭建模型"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#搭建模型"}},[t._v("#")]),t._v(" 搭建模型")]),t._v(" "),s("p",[t._v("这是模型构建的通常案例")]),t._v(" "),s("ol",[s("li",[t._v("定义sequential模型")]),t._v(" "),s("li",[t._v("开始添加核心层")]),t._v(" "),s("li",[t._v("首先，将添加一个Conv2D层，它有16个node，内核大小为（3,3）。也可以在这里尝试不同的值，如32，128等。此外，我们必须指定输入形状"),s("code",[t._v("input_shape")]),t._v("，激活我们将采取"),s("code",[t._v("'relu'")]),t._v("。")]),t._v(" "),s("li",[t._v("我们将再次重复这个组合，因为2比1好。我们还可以添加3个或更多的卷积层，不过添加的层越多，训练所需的时间就越多。")]),t._v(" "),s("li",[t._v("添加dropout层")]),t._v(" "),s("li",[t._v("为了得到我们的结果，我们将添加最后的Dense层。激活函数可以是"),s("code",[t._v("sigmoid")]),t._v("或"),s("code",[t._v("softmax")]),t._v("（如果需要，请使用sigmoid，否则请使用softmax）。")]),t._v(" "),s("li",[t._v("最后我们将编译模型，这里有三个参数 "),s("code",[t._v("Loss")]),t._v(", "),s("code",[t._v("Optimizer")]),t._v(", "),s("code",[t._v("Metrics")]),t._v("。")])]),t._v(" "),s("h3",{attrs:{id:"step1-convolution-operation-pooling"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#step1-convolution-operation-pooling"}},[t._v("#")]),t._v(" Step1-Convolution Operation & Pooling")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("input_shape")]),t._v(" 让输入的图片有一致的格式， 3 是指 R/G/B 颜色三通道")]),t._v(" "),s("li",[s("code",[t._v("activation function")]),t._v(" 激活函数设置为 "),s("code",[t._v("ReLU")])]),t._v(" "),s("li",[t._v("用 "),s("code",[t._v("max pooling")]),t._v("，作用是降躁跟减少运算资源，选择滤波器大小为2*2，stride（滑动间隔）与滤波器长度一致。")]),t._v(" "),s("li",[t._v("每个Convolution Operation 搭配 一个 Pooling层")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#搭建CNN模型")]),t._v("\nvani_model "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Sequential"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'same'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" input_shape "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'same'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("MaxPooling2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pool_size "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" strides"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("同理，搭建第二三四层")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 第二层")]),t._v("\nvani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'same'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'same'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("MaxPooling2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pool_size "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" strides"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#第三层")]),t._v("\nvani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'same'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'same'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("MaxPooling2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pool_size "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("strides"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#第四层")]),t._v("\nvani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'same'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'same'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("MaxPooling2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pool_size "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("strides"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"step2-dropout"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#step2-dropout"}},[t._v("#")]),t._v(" Step2-Dropout")]),t._v(" "),s("p",[t._v("设置"),s("code",[t._v("rate")]),t._v("为0.3")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#设置dropout层")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# tf.keras.layers.Dropout(rate, noise_shape=None, seed=None, **kwargs)")]),t._v("\nvani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dropout"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"step3-convolution-operation-pooling"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#step3-convolution-operation-pooling"}},[t._v("#")]),t._v(" Step3-Convolution Operation & Pooling")]),t._v(" "),s("p",[t._v("再搭建一个卷积池化层")]),t._v(" "),s("div",{staticClass:"language-PYTHON extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("vani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("256")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'same'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("256")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" padding "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'same'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("MaxPooling2D"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pool_size "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("strides"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nvani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dropout"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"step4-flatten"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#step4-flatten"}},[t._v("#")]),t._v(" Step4-Flatten")]),t._v(" "),s("p",[t._v("将 feature maps 摊平放入一个向量中")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("vani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Flatten"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"step5-全连接层fully-connected-networks"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#step5-全连接层fully-connected-networks"}},[t._v("#")]),t._v(" Step5-全连接层Fully Connected Networks")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("vani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dense"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nvani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dropout"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nvani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dense"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'softmax'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("前面一开始建了 16 个 feature maps, 之后在conv, max-pool, flatten 会得到大型的vector(有很多input node)，所以这边隐藏层的 node 数目不能太小，太小可能模型不好，太大可能会花费大量计算资源。先设置为512个node。")]),t._v(" "),s("p",[t._v("最终获得模型：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("vani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("summary"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 224, 224, 16)      448       \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 224, 224, 16)      2320      \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 112, 112, 16)      0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 112, 112, 32)      4640      \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 112, 112, 32)      9248      \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 56, 56, 32)        0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 56, 56, 64)        18496     \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 56, 56, 64)        36928     \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 28, 28, 64)        0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 28, 28, 128)       73856     \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 28, 28, 128)       147584    \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 14, 14, 128)       0         \n_________________________________________________________________\ndropout (Dropout)            (None, 14, 14, 128)       0         \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 14, 14, 256)       295168    \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 14, 14, 256)       590080    \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 7, 7, 256)         0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 7, 7, 256)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 12544)             0         \n_________________________________________________________________\ndense (Dense)                (None, 512)               6423040   \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 2)                 1026      \n=================================================================\nTotal params: 7,602,834\nTrainable params: 7,602,834\nNon-trainable params: 0\n_________________________________________________________________\n'''")]),t._v("\n")])])]),s("h3",{attrs:{id:"step6-compiling-the-cnn"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#step6-compiling-the-cnn"}},[t._v("#")]),t._v(" Step6-Compiling the CNN")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("optimizer")]),t._v(" ：如果你对机器学习的数学有一点了解，你可能更熟悉局部极小值或全局极小值或代价函数。为了最小化代价函数，我们使用了不同的方法，例如梯度下降法，随机梯度下降法。所以这些是调用优化器。我们在这里使用一个默认值，使用 "),s("code",[t._v("adam")]),t._v(" （["),s("a",{attrs:{href:"https://ithelp.ithome.com.tw/articles/10204032",target:"_blank",rel:"noopener noreferrer"}},[t._v("精進魔法] Optimization：優化深度學習模型的技巧（中）")]),t._v("提到 Adam 是常用的方法。）")]),t._v(" "),s("li",[s("code",[t._v("loss")]),t._v(" ：为了使模型更好，我们要么最小化损失，要么最大限度地提高精度。NN总是使损失最小化。为了测量它，我们可以使用不同的公式，如“分类交叉熵”（"),s("code",[t._v("'categorical_crossentropy'")]),t._v("）或“二进制交叉熵”（"),s("code",[t._v("'binary_crossentropy'")]),t._v("）。这里使用 "),s("code",[t._v("categorical_crossentropy")]),t._v("。")]),t._v(" "),s("li",[s("code",[t._v("metrics")]),t._v("：这表示模型的度量。可以是精度"),s("code",[t._v("'accuracy'")]),t._v("或其他指标。")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("loss "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'categorical_crossentropy'")]),t._v("\nopt "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optimizers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Adam"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("learning_rate"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.0001")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("beta_1"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.9")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" beta_2"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.999")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("epsilon"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("07")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmetrics "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'accuracy'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\nvani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" loss"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" optimizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" opt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" metrics"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"step7-training"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#step7-training"}},[t._v("#")]),t._v(" Step7-training")]),t._v(" "),s("p",[t._v("输入训练和验证数据集，设置epochs为15，进行训练。")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("vani_history "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vani_train_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" epochs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                          validation_data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vani_valid_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                          validation_steps"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" valid_images"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("batch_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                          steps_per_epoch"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_images"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("batch_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"可选optional-step8-plot-model"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#可选optional-step8-plot-model"}},[t._v("#")]),t._v(" （可选optional）Step8-plot model")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("plot_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" show_shapes "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("expand_nested "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("dpi "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("80")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nSVG"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model_to_dot"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("create"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("prog"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'dot'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'svg'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"http://test-123-imagebed.oss-cn-beijing.aliyuncs.com/img/image-20210625104057205.png",alt:"image-20210625104057205"}})]),t._v(" "),s("h2",{attrs:{id:"评估模型"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#评估模型"}},[t._v("#")]),t._v(" 评估模型")]),t._v(" "),s("ul",[s("li",[t._v("作出训练过程中的epochs-accuracy、epochs-loss曲线图")]),t._v(" "),s("li",[t._v("求模型的平均准确率mAP")]),t._v(" "),s("li",[t._v("使用模型进行预测(predice)，查看效果")])]),t._v(" "),s("h3",{attrs:{id:"学习曲线"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#学习曲线"}},[t._v("#")]),t._v(" 学习曲线")]),t._v(" "),s("p",[t._v("这部分主要参考："),s("a",{attrs:{href:"https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/",target:"_blank",rel:"noopener noreferrer"}},[t._v("如何使用学习曲线诊断机器学习模型性能")])]),t._v(" "),s("p",[t._v("学习曲线的形状和动态可用于诊断机器学习模型的行为，进而可能建议为改进学习和/或性能而进行的配置更改类型。")]),t._v(" "),s("p",[t._v("您可能会在学习曲线中观察到三种常见的动态；他们是：")]),t._v(" "),s("ul",[s("li",[t._v("欠拟合")]),t._v(" "),s("li",[t._v("过拟合")]),t._v(" "),s("li",[t._v("好身材")])]),t._v(" "),s("h4",{attrs:{id:"欠拟合"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#欠拟合"}},[t._v("#")]),t._v(" 欠拟合")]),t._v(" "),s("p",[t._v("欠拟合是指模型无法学习训练数据集。")]),t._v(" "),s("blockquote",[s("p",[t._v("当模型无法在训练集上获得足够低的误差值时，就会发生欠拟合")])]),t._v(" "),s("p",[t._v("只能从训练损失的学习曲线中识别欠拟合模型。它可能会显示一条平线或相对较高损失的噪声值，表明模型根本无法学习训练数据集。下面提供了一个示例，当模型没有适合数据集复杂性的容量时，这很常见。")]),t._v(" "),s("img",{staticStyle:{zoom:"50%"},attrs:{src:"http://test-123-imagebed.oss-cn-beijing.aliyuncs.com/img/image-20210625105202086.png",alt:"image-20210625105202086"}}),t._v(" "),s("h2",{attrs:{id:"错误记录"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#错误记录"}},[t._v("#")]),t._v(" 错误记录")]),t._v(" "),s("h3",{attrs:{id:""}},[s("a",{staticClass:"header-anchor",attrs:{href:"#"}},[t._v("#")])]),t._v(" "),s("p",[t._v("在模型训练中")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# fitting the model for training dataset")]),t._v("\nvani_history "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" vani_model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_generator"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("steps_per_epoch"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("STEP_SIZE_TRAIN"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("epochs "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" epochs"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                          validation_data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" validation_generator"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                          validation_steps"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("STEP_SIZE_VALID"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("output：\nValueError"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Error when checking target"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" expected dense_2 to have shape "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" but got array "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" shape "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"http://test-123-imagebed.oss-cn-beijing.aliyuncs.com/img/image-20210625112006372.png",alt:""}})]),t._v(" "),s("p",[t._v("在这个模型中dense_2表现为模型的期望分类类别，模型中预测有2种标签，但最后只有一种。所以这里报错"),s("code",[t._v("expected dense_2 to have shape (2,) but got array with shape (1,)")])]),t._v(" "),s("p",[t._v("那么就检查是哪里变得只有一种标签了，后来发现，是cats的训练文件夹下根本没有图片（超级大失误），因此重新解压数据集。")]),t._v(" "),s("p",[t._v("但发现也并不是这个原因，通过查询Stack Overflow的相关问题："),s("a",{attrs:{href:"https://stackoverflow.com/questions/49392972/error-when-checking-target-expected-dense-3-to-have-shape-3-but-got-array-wi",target:"_blank",rel:"noopener noreferrer"}},[t._v("Error when checking target: expected dense_3 to have shape (3,) but got array with shape (1,)")]),t._v("，发现原来是最开始的"),s("code",[t._v("train_generator")]),t._v("出现了问题")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("train_generator "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_datagen"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flow_from_directory"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    train_data_dir"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    target_size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_height"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_width"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    batch_size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("batch_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    class_mode"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'binary'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("在这里的"),s("code",[t._v("class_mode='binary'")]),t._v("产生的1D标签，但我们设置的dense层是2D标签分类，所以将编码模式变成"),s("code",[t._v('"categorical"')]),t._v("，采用 2D one-hot 编码标签就解决问题了。或者将Dense层的out_dim设置为1。")]),t._v(" "),s("p",[t._v("关于编码模式可以查看"),s("a",{attrs:{href:"https://keras.io/api/preprocessing/image/#flowfromdirectory-method",target:"_blank",rel:"noopener noreferrer"}},[t._v("keras Image data preprocessing API")]),t._v("。")]),t._v(" "),s("h3",{attrs:{id:"模型训练过程中-accuracy会突然下降然后又恢复到原来的趋势"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#模型训练过程中-accuracy会突然下降然后又恢复到原来的趋势"}},[t._v("#")]),t._v(" 模型训练过程中，accuracy会突然下降然后又恢复到原来的趋势")]),t._v(" "),s("p",[t._v("在神经网络的训练过程中，正常情况下整体的过程是一个Loss不断下降的过程，然而在训练过程中往往存在一些batch_size输入时，前向传播得到的Loss突然变大的情景。")]),t._v(" "),s("p",[t._v("在训练神经网络过程中，经过softmax函数输出概率后，当当前最大概率对应的类别与训练标签一致时，损失Loss往往较小，而经过softmax输出时，训练标签所对应的概率softmax概率较小，此时的Loss计算结果往往很大。对此引出两个解决问题的思路：")]),t._v(" "),s("p",[t._v("1.可能是训练标签未进行很好的标注，假设当前是一个二分类问题，我们需要提前定义的标签类别为0,1。假设一个标签被错误的定义为2，导致此batch输入无论怎么训练得到的结果都是错误的分类，导致Loss过大情形，此时需要我们可以通过重新制作训练标签来解决。")]),t._v(" "),s("p",[t._v("2.可以通过设定一些条件来对Loss进行约束，在训练过程中如果出现Loss增大程度超出了一定的阈值，可以将此时的Loss强制设为0，这样在反向传播时就不会对权重和bias系数的迭代更新做贡献。\n原文链接：https://blog.csdn.net/bigFatCat_Tom/article/details/90759119")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://blog.csdn.net/mooneve/article/details/97615814?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-5.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-5.control",target:"_blank",rel:"noopener noreferrer"}},[t._v("CSDN：网络训练时train/val loss出现周期性剧增原因")]),t._v("中认为通过观察loss具体的数值变化，发现每次loss剧增都发生在一个epoch的最后一轮。")]),t._v(" "),s("p",[t._v("通过分析，我判断出这是由于我的图片数量不能被batch size整除导致的。")]),t._v(" "),s("p",[t._v("我的train/val分别有2450,610张图片，batch size为32，这使得每个epoch最后一次迭代的图片很少。")]),t._v(" "),s("p",[t._v("由于每轮训练的loss是所有图片loss的平均。当图片数量很少时，随机性更大，平均下来loss就更高了。")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://datascience.stackexchange.com/questions/58731/what-can-be-the-cause-of-a-sudden-explosion-in-the-loss-when-training-a-cnn-dee",target:"_blank",rel:"noopener noreferrer"}},[t._v("stackexchange：What can be the cause of a sudden explosion in the loss when training a CNN (Deeplab)")])]),t._v(" "),s("p",[t._v("有人认为一种可能的原因可能是某些权重或梯度的数值不稳定。为确保不会发生这种情况，建议在网络的某些关键部分进行剪辑。")]),t._v(" "),s("img",{staticStyle:{zoom:"50%"},attrs:{src:"http://test-123-imagebed.oss-cn-beijing.aliyuncs.com/img/image-20210629134753898.png",alt:"image-20210629134753898"}}),t._v(" "),s("p",[t._v("在另一个相关问题里")]),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"http://test-123-imagebed.oss-cn-beijing.aliyuncs.com/img/image-20210629134820747.png",alt:"image-20210629134820747"}}),t._v(" "),s("p",[t._v("回答者认为似乎是由于某种原因梯度爆炸了。考虑使用渐变剪裁来防止这种情况发生。尝试使用参数"),s("code",[t._v("clipnorm")]),t._v("或"),s("code",[t._v("clipvalue")]),t._v("通过将优化器定义修改为：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("optimizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SGD"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lr"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.01")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" momentum"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.9")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" nesterov"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" clipnorm"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("或是")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("optimizer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" SGD"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lr"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.01")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" momentum"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.9")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" nesterov"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" clipvalue"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("最后提问者表示降低学习率解决了这个问题。")]),t._v(" "),s("p",[t._v("有关渐变剪裁的使用可以查看"),s("a",{attrs:{href:"https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/",target:"_blank",rel:"noopener noreferrer"}},[t._v("How to Avoid Exploding Gradients With Gradient Clipping")])]),t._v(" "),s("h2",{attrs:{id:"参考资料"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#参考资料"}},[t._v("#")]),t._v(" 参考资料")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://blog.csdn.net/u012905422/article/details/52463324?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-5.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-5.control",target:"_blank",rel:"noopener noreferrer"}},[t._v("CSDN：CNN中的不同种类层简介")]),t._v("：介绍了CNN的不同种类层")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://towardsdatascience.com/covolutional-neural-network-cb0883dd6529",target:"_blank",rel:"noopener noreferrer"}},[t._v("TDS： "),s("em",[t._v("Convolutional Neural Network")])])]),t._v(" "),s("p",[s("a",{attrs:{href:"https://blog.csdn.net/dcrmg/article/details/81255498?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control",target:"_blank",rel:"noopener noreferrer"}},[t._v("CSDN：卷积神经网络特征图可视化（自定义网络和VGG网络）")]),t._v("：CNN网络特征图的可视化")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53",target:"_blank",rel:"noopener noreferrer"}},[t._v("Towarddatascience:A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way")])]),t._v(" "),s("p",[s("a",{attrs:{href:"https://ithelp.ithome.com.tw/articles/10205389",target:"_blank",rel:"noopener noreferrer"}},[t._v("[實戰系列] 使用 Keras 搭建一個 CNN 魔法陣（模型）")]),t._v("：使用keras构建CNN模型")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://zh-v2.d2l.ai/chapter_convolutional-modern/index.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("动手学深度学习2：现代神经网络")])]),t._v(" "),s("p",[s("a",{attrs:{href:"https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-photos-of-dogs-and-cats/",target:"_blank",rel:"noopener noreferrer"}},[t._v("如何对狗和猫的照片进行分类（准确率为 97%）")]),t._v("：搭建了baseline CNN，然后使用VGG16、VGG19")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://keras.io/api/layers/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Keras 层 API")])]),t._v(" "),s("p",[s("a",{attrs:{href:"https://data-flair.training/blogs/cats-dogs-classification-deep-learning-project-beginners/",target:"_blank",rel:"noopener noreferrer"}},[t._v("使用 CNN Keras 进行猫与狗分类（准确率为 98.7%）——面向初学者的深度学习项目")]),t._v("：介绍了CNN模型和利用模型文件搭建gui面板检测")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://www.codenong.com/54702212/",target:"_blank",rel:"noopener noreferrer"}},[t._v("关于python：Keras：rescale = 1. / 255与preprocessing_function = preprocess_input-使用哪个？")])]),t._v(" "),s("p",[s("a",{attrs:{href:"https://stackoverflow.com/questions/62348246/concatenate-object-has-no-attribute-flatten",target:"_blank",rel:"noopener noreferrer"}},[t._v("'Concatenate' object has no attribute 'Flatten'")])]),t._v(" "),s("h3",{attrs:{id:"应用接口"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#应用接口"}},[t._v("#")]),t._v(" 应用接口")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://keras.io/api/models/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Keras模型 API")])])])}),[],!1,null,null,null);a.default=n.exports}}]);